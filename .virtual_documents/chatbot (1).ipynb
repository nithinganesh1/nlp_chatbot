import nltk #to process natural language
import numpy as np
import random # to generte random responses to feel less repetitive
import string # to do string manipulations, because sometimes we need to remove punctuations ,need to convert it in lowercase etc.


f = open('Machine learning.txt', 'r', errors='ignore')
raw = f.read()


raw=raw.lower()# convert all strings to lowercase
nltk.download('punkt')# Punkt is a pre-trained model that helps in sentence tokenization, i.e., splitting a text into sentences
nltk.download('wordnet')#WordNet is a large database of English words that groups words into sets of synonyms 
sentence_tokens = nltk.sent_tokenize(raw)
word_tokens = nltk.word_tokenize(raw)
print(sentence_tokens[:5])
print(word_tokens[:5])


#normalize the text by removing punctuation and reducing words to their base or root form (lemmatization)
lemmer = nltk.stem.WordNetLemmatizer()

remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation) ##This dictionary will be used to remove punctuation from the text.

def lem_tokens(tokens): #takes a list of tokens (words) as input and returns a list of lemmatized tokens.
    return [lemmer.lemmatize(token) for token in tokens]

def lem_normalize(text):#This function, lem_normalize, normalizes the input text by performing  translate(),lower(), tokenize()
    return lem_tokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))

# translate() used to modify a string by replacing characters based on a specified translation table
#text.lower() converts all characters in the text to lowercase, ensuring uniformity.
#translate(remove_punct_dict) removes any punctuation from the text using the remove_punct_dict created earlier.
#nltk.word_tokenize() splits the cleaned text into individual words (tokens).
#lem_tokens() is then called to lemmatize each token in the list.


GREETING_INPUTS = ('hello', 'hi', 'greetings', 'sup', 'what\'s up', 'hey',)
GREETING_RESPONSES = ['hi', 'hey', '*nods*', 'hi there', 'hello', 'I am glad! You are talking to me']

def greeting(sentence):
    for word in sentence.split():
        if word.lower() in GREETING_INPUTS:
            return random.choice(GREETING_RESPONSES)


#generates a response to a user's input based on the similarity between the user's query and the chatbot's available data.


from sklearn.feature_extraction.text import TfidfVectorizer #TF-IDF is a statistical measure used to evaluate the importance of a 
#word in a document relative to a collection of documents (corpus). TF-IDF allows the model to focus on the most important 
#words when comparing the user input to the available data.
from sklearn.metrics.pairwise import cosine_similarity #Cosine similarity is widely used in text processing to measure
# the similarity between two documents (or sentences) represented as vectors


def response(user_response):
    robo_response = ''
    # Temporarily add user_response for similarity calculation
    temp_tokens = sentence_tokens + [user_response]

    vectorizer = TfidfVectorizer(tokenizer=lem_normalize, stop_words='english')
    tfidf = vectorizer.fit_transform(temp_tokens)  # Use the temporary token list

    values = cosine_similarity(tfidf[-1], tfidf[:-1])  # Compare with all except the last one (user response)
    idx = values.argsort()[0][-1]  # Find the most similar sentence
    flat = values.flatten()
    flat.sort()
    req_tfidf = flat[-1]  # Get the highest similarity score

    if req_tfidf == 0:
        robo_response = '{} Sorry, I don\'t understand you'.format(robo_response)
    else:
        robo_response = robo_response + sentence_tokens[idx]
    return robo_response



pip install pyttsx3


import pyttsx3
engine = pyttsx3.init()
# Set the rate (speed) of speech
engine.setProperty('rate', 150)  # 150 words per minute

# Set the volume (0.0 to 1.0)
engine.setProperty('volume', 0.9)  # 90% volume
engine.say("Hello, how can I assist you today?")
engine.runAndWait()


flag = True
print('BOT: My name is Robo, I will answer your questions about Machine Learning. If you want to exit, type Bye')

interactions = [
    'hi',
    'what is Machine Learning?',
    'What are the different types of Machine Learning algorithms?',
    'What is the difference between supervised and unsupervised learning',
    'What is the difference between classification and regression?',
    'machine learning algorithms?'
    'sounds awesome',
    'bye',
]
while flag:
    user_response = input("User: ")
    user_response = user_response.lower()
    if user_response != 'bye':
        if user_response == 'thanks' or user_response == 'thank you':
            flag = False
            print('BOT: You are welcome...')
        elif greeting(user_response) != None:
            print('ROBO: {}'.format(greeting(user_response)))
            engine.say(format(greeting(user_response)))
            engine.runAndWait()
        else:
            print('ROBO: ', end='')
            print(response(user_response))
            sentence_tokens.remove(user_response)
            engine.say(format(response(user_response)))
            engine.runAndWait()
    else:
        flag = False
        print('BOT: bye!')



